"""
Redis ì—…ë°ì´íŠ¸ ìµœì í™” ëª¨ë“ˆ
- Patternë³„ ìµœì í™”ëœ ì—…ë°ì´íŠ¸ ì „ëµ
- ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„° í¬í•¨
"""
import sys
import os

# tests/ì—ì„œ í•œ ë‹¨ê³„ ìœ„ì¸ backend_serverë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import time
from datetime import datetime, timezone
from typing import Optional, Dict, Any, Literal
from functools import wraps
from collections import defaultdict

from database.redis_conn import RedisConnection
from utils.debug_print import debug_print


class RedisUpdateMetrics:
    """Redis ì—…ë°ì´íŠ¸ ì„±ëŠ¥ ì¸¡ì •"""

    def __init__(self):
        self.metrics = defaultdict(lambda: {
            "count": 0,
            "total_time": 0.0,
            "total_bytes_read": 0,
            "total_bytes_written": 0,
            "min_time": float('inf'),
            "max_time": 0.0
        })

    def record(
        self,
        operation: str,
        duration: float,
        bytes_read: int = 0,
        bytes_written: int = 0
    ):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        m = self.metrics[operation]
        m["count"] += 1
        m["total_time"] += duration
        m["total_bytes_read"] += bytes_read
        m["total_bytes_written"] += bytes_written
        m["min_time"] = min(m["min_time"], duration)
        m["max_time"] = max(m["max_time"], duration)

    def get_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        summary = {}
        for operation, data in self.metrics.items():
            if data["count"] == 0:
                continue

            avg_time = data["total_time"] / data["count"]
            avg_bytes_read = data["total_bytes_read"] / data["count"]
            avg_bytes_written = data["total_bytes_written"] / data["count"]

            summary[operation] = {
                "í˜¸ì¶œ_íšŸìˆ˜": data["count"],
                "ì´_ì†Œìš”ì‹œê°„_ms": round(data["total_time"] * 1000, 2),
                "í‰ê· _ì†Œìš”ì‹œê°„_ms": round(avg_time * 1000, 2),
                "ìµœì†Œ_ì†Œìš”ì‹œê°„_ms": round(data["min_time"] * 1000, 2),
                "ìµœëŒ€_ì†Œìš”ì‹œê°„_ms": round(data["max_time"] * 1000, 2),
                "ì´_ì½ê¸°_ë°”ì´íŠ¸": data["total_bytes_read"],
                "í‰ê· _ì½ê¸°_ë°”ì´íŠ¸": round(avg_bytes_read, 2),
                "ì´_ì“°ê¸°_ë°”ì´íŠ¸": data["total_bytes_written"],
                "í‰ê· _ì“°ê¸°_ë°”ì´íŠ¸": round(avg_bytes_written, 2),
            }
        return summary

    def reset(self):
        """ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
        self.metrics.clear()


# ì „ì—­ ë©”íŠ¸ë¦­ ì¸ìŠ¤í„´ìŠ¤
_global_metrics = RedisUpdateMetrics()


def measure_redis_update(operation_name: str):
    """Redis ì—…ë°ì´íŠ¸ ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.perf_counter()

            # í•¨ìˆ˜ ì‹¤í–‰
            result = await func(*args, **kwargs)

            # ì†Œìš” ì‹œê°„ ê¸°ë¡
            duration = time.perf_counter() - start_time

            # ë©”íŠ¸ë¦­ ê¸°ë¡ (ë°”ì´íŠ¸ í¬ê¸°ëŠ” resultì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•˜ë©´ ì¶”ì¶œ)
            bytes_info = {}
            if isinstance(result, dict):
                bytes_info = {
                    "bytes_read": result.get("_bytes_read", 0),
                    "bytes_written": result.get("_bytes_written", 0)
                }

            _global_metrics.record(
                operation_name,
                duration,
                bytes_info.get("bytes_read", 0),
                bytes_info.get("bytes_written", 0)
            )

            return result
        return wrapper
    return decorator


def get_metrics_summary() -> Dict[str, Any]:
    """ì „ì—­ ë©”íŠ¸ë¦­ ìš”ì•½ ì¡°íšŒ"""
    return _global_metrics.get_summary()


def reset_metrics():
    """ì „ì—­ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
    _global_metrics.reset()


class OptimizedRedisUpdater:
    """
    íŒ¨í„´ë³„ ìµœì í™”ëœ Redis ì—…ë°ì´íŠ¸

    ê°œì„ ì :
    1. Sequential: step ë‹¨ìœ„ë¡œ í•´ì‹œ í•„ë“œ ì—…ë°ì´íŠ¸ (HSET ì‚¬ìš©)
    2. Parallel: group ë‹¨ìœ„ë¡œ í•´ì‹œ í•„ë“œ ì—…ë°ì´íŠ¸ (HSET ì‚¬ìš©)
    3. ì „ì²´ ë°ì´í„° GET/SET ìµœì†Œí™”
    """

    def __init__(self, redis_connection: RedisConnection):
        self.redis = redis_connection.client

    # ========================================
    # Sequential íŒ¨í„´ìš© ìµœì í™”
    # ========================================

    @measure_redis_update("sequential_step_update")
    async def update_sequential_step_status(
        self,
        simulation_id: int,
        execution_id: int,
        step_order: int,
        status: str,
        progress: Optional[float] = None,
        current_repeat: Optional[int] = None,
        total_repeats: Optional[int] = None,
        autonomous_agents: Optional[int] = None,
        error: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ìˆœì°¨ íŒ¨í„´: Step ìƒíƒœë§Œ ì—…ë°ì´íŠ¸ (ë¶€ë¶„ ì—…ë°ì´íŠ¸)

        ê°œì„  ë°©ì‹:
        - ì „ì²´ ë°ì´í„° ëŒ€ì‹  stepDetails ë°°ì—´ë§Œ ì¡°íšŒ
        - í•´ë‹¹ stepë§Œ ìˆ˜ì • í›„ ë‹¤ì‹œ ì €ì¥
        """
        execution_key = f"simulation:{simulation_id}:execution:{execution_id}"
        steps_hash = f"{execution_key}:steps"
        now = datetime.now(timezone.utc)

        step_field = f"step:{step_order}"

        # Try to get the individual step from a hash (partial-update storage)
        try:
            step_raw = await self.redis.hget(steps_hash, step_field)
        except AttributeError:
            # If client doesn't support hget (unlikely), fallback to full GET
            step_raw = None

        # If step not present in hash, try to migrate from full execution JSON
        if not step_raw:
            full_raw = await self.redis.get(execution_key)
            if not full_raw:
                # Nothing to update
                return {"_bytes_read": 0, "_bytes_written": 0}

            full_raw_text = full_raw.decode('utf-8') if isinstance(full_raw, bytes) else full_raw
            try:
                full_obj = json.loads(full_raw_text)
            except json.JSONDecodeError:
                return {"_bytes_read": len(full_raw_text.encode('utf-8')), "_bytes_written": 0}

            # Migrate each step into hash fields
            step_details = full_obj.get("stepDetails", [])
            for s in step_details:
                field = f"step:{s.get('stepOrder')}"
                await self.redis.hset(steps_hash, field, json.dumps(s))

            # after migration, get the specific step
            step_raw = await self.redis.hget(steps_hash, step_field)

        if not step_raw:
            debug_print(f"âš ï¸ Step {step_order}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return {"_bytes_read": 0, "_bytes_written": 0}

        # Normalize raw bytes/str
        step_text = step_raw.decode('utf-8') if isinstance(step_raw, bytes) else step_raw
        bytes_read = len(step_text.encode('utf-8'))

        try:
            step_obj = json.loads(step_text)
        except json.JSONDecodeError:
            return {"_bytes_read": bytes_read, "_bytes_written": 0}

        # Capture previous status BEFORE mutating for accurate meta update
        prev_status = step_obj.get("status")

        # Update step object in-place
        step_obj["status"] = status
        if progress is not None:
            step_obj["progress"] = progress
        if current_repeat is not None:
            step_obj["currentRepeat"] = current_repeat
        if total_repeats is not None:
            step_obj["totalRepeats"] = total_repeats
        if autonomous_agents is not None:
            step_obj["autonomousAgents"] = autonomous_agents

        # timestamp updates
        if status == "RUNNING" and not step_obj.get("startedAt"):
            step_obj["startedAt"] = now.isoformat()
        elif status == "COMPLETED":
            step_obj["completedAt"] = now.isoformat()
        elif status == "FAILED":
            step_obj["failedAt"] = now.isoformat()
            step_obj["error"] = error
        elif status == "STOPPED":
            step_obj["stoppedAt"] = now.isoformat()
            step_obj["error"] = error

        # Write back only the updated step into the hash
        updated_step = json.dumps(step_obj)
        bytes_written = len(updated_step.encode('utf-8'))
        await self.redis.hset(steps_hash, step_field, updated_step)

        # Update meta incrementally using prev_status to determine transitions
        try:
            meta_raw = await self.redis.hget(steps_hash, "meta")
            if meta_raw:
                meta_text = meta_raw.decode('utf-8') if isinstance(meta_raw, bytes) else meta_raw
                try:
                    meta = json.loads(meta_text)
                except Exception:
                    meta = None
            else:
                meta = None

            if meta is None:
                # Initialize meta from migration data (hgetall fallback)
                try:
                    all_fields = await self.redis.hgetall(steps_hash)
                    total_steps = sum(1 for k in all_fields.keys() if k.startswith("step:"))
                    completed_steps = 0
                    running_step = 0
                    for k, v in all_fields.items():
                        if not k.startswith("step:"):
                            continue
                        val_text = v.decode('utf-8') if isinstance(v, bytes) else v
                        try:
                            so = json.loads(val_text)
                        except Exception:
                            continue
                        if so.get("status") == "COMPLETED":
                            completed_steps += 1
                        if so.get("status") == "RUNNING":
                            running_step = so.get("stepOrder", running_step)

                    meta = {"totalSteps": total_steps, "completedSteps": completed_steps, "currentStep": running_step}
                except AttributeError:
                    # cannot hgetall; set conservative defaults
                    meta = {"totalSteps": 0, "completedSteps": 0, "currentStep": 0}

            # Use prev_status to detect meaningful transitions
            # If step moved into COMPLETED from a non-COMPLETED state -> increment completedSteps
            if status == "COMPLETED" and prev_status != "COMPLETED":
                meta["completedSteps"] = meta.get("completedSteps", 0) + 1

            # If step moved into RUNNING from a non-RUNNING state -> set currentStep
            if status == "RUNNING" and prev_status != "RUNNING":
                meta["currentStep"] = step_order

            # If step was RUNNING and moved away (e.g., STOPPED/COMPLETED),
            # clear currentStep only if it referred to this step
            if prev_status == "RUNNING" and status != "RUNNING":
                if meta.get("currentStep") == step_order:
                    # Try to find another running step or reset to 0
                    # Simple approach: unset to 0 (consumer can recalc if needed)
                    meta["currentStep"] = 0

            await self.redis.hset(steps_hash, "meta", json.dumps(meta))
        except AttributeError:
            # client doesn't support hget/hset; ignore
            pass

        return {"_bytes_read": bytes_read, "_bytes_written": bytes_written}

    @measure_redis_update("sequential_simulation_update")
    async def update_sequential_simulation_status(
        self,
        simulation_id: int,
        execution_id: int,
        status: str,
        reason: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ìˆœì°¨ íŒ¨í„´: ì „ì²´ ì‹œë®¬ë ˆì´ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        """
        execution_key = f"simulation:{simulation_id}:execution:{execution_id}"
        now = datetime.now(timezone.utc)

        # í˜„ì¬ ìƒíƒœ ì¡°íšŒ
        raw_data = await self.redis.get(execution_key)
        if not raw_data:
            return {"_bytes_read": 0, "_bytes_written": 0}

        bytes_read = len(raw_data) if isinstance(raw_data, bytes) else len(raw_data.encode('utf-8'))

        if isinstance(raw_data, bytes):
            raw_data = raw_data.decode("utf-8")

        try:
            current_status = json.loads(raw_data)
        except json.JSONDecodeError:
            return {"_bytes_read": bytes_read, "_bytes_written": 0}

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        current_status["status"] = status

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸
        current_status["timestamps"]["lastUpdated"] = now.isoformat()
        if status == "COMPLETED":
            current_status["timestamps"]["completedAt"] = now.isoformat()
        elif status == "FAILED":
            current_status["timestamps"]["failedAt"] = now.isoformat()
        elif status == "STOPPED":
            current_status["timestamps"]["stoppedAt"] = now.isoformat()

        # ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
        if reason:
            current_status["message"] = reason

        # Redisì— ì €ì¥
        updated_data = json.dumps(current_status)
        bytes_written = len(updated_data.encode('utf-8'))

        await self.redis.set(execution_key, updated_data)

        # Primary keyë„ ë™ì¼í•˜ê²Œ ì—…ë°ì´íŠ¸
        primary_key = f"simulation:{simulation_id}"
        await self.redis.set(primary_key, updated_data)

        return {
            "_bytes_read": bytes_read,
            "_bytes_written": bytes_written * 2
        }

    # ========================================
    # Parallel íŒ¨í„´ìš© ìµœì í™”
    # ========================================

    @measure_redis_update("parallel_group_update")
    async def update_parallel_group_status(
        self,
        simulation_id: int,
        execution_id: int,
        group_id: int,
        status: str,
        progress: Optional[float] = None,
        current_repeat: Optional[int] = None,
        total_repeats: Optional[int] = None,
        autonomous_agents: Optional[int] = None,
        error: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ë³‘ë ¬ íŒ¨í„´: Group ìƒíƒœë§Œ ì—…ë°ì´íŠ¸ (ë¶€ë¶„ ì—…ë°ì´íŠ¸)

        ê°œì„  ë°©ì‹:
        - ì „ì²´ ë°ì´í„° ëŒ€ì‹  groupDetails ë°°ì—´ë§Œ ì¡°íšŒ
        - í•´ë‹¹ groupë§Œ ìˆ˜ì • í›„ ë‹¤ì‹œ ì €ì¥
        """
        execution_key = f"simulation:{simulation_id}:execution:{execution_id}"
        groups_hash = f"{execution_key}:groups"
        now = datetime.now(timezone.utc)

        group_field = f"group:{group_id}"

        # Try to get the individual group from a hash (partial-update storage)
        try:
            group_raw = await self.redis.hget(groups_hash, group_field)
        except AttributeError:
            group_raw = None

        # If group not present in hash, try to migrate from full execution JSON
        if not group_raw:
            full_raw = await self.redis.get(execution_key)
            if not full_raw:
                return {"_bytes_read": 0, "_bytes_written": 0}

            full_raw_text = full_raw.decode('utf-8') if isinstance(full_raw, bytes) else full_raw
            try:
                full_obj = json.loads(full_raw_text)
            except json.JSONDecodeError:
                return {"_bytes_read": len(full_raw_text.encode('utf-8')), "_bytes_written": 0}

            # Migrate each group into hash fields
            group_details = full_obj.get("groupDetails", [])
            for g in group_details:
                field = f"group:{g.get('groupId')}"
                await self.redis.hset(groups_hash, field, json.dumps(g))

            # after migration, get the specific group
            group_raw = await self.redis.hget(groups_hash, group_field)

        if not group_raw:
            debug_print(f"âš ï¸ Group {group_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return {"_bytes_read": 0, "_bytes_written": 0}

        # Normalize raw bytes/str
        group_text = group_raw.decode('utf-8') if isinstance(group_raw, bytes) else group_raw
        bytes_read = len(group_text.encode('utf-8'))

        try:
            group_obj = json.loads(group_text)
        except json.JSONDecodeError:
            return {"_bytes_read": bytes_read, "_bytes_written": 0}

        # Update group object in-place
        group_obj["status"] = status
        if progress is not None:
            group_obj["progress"] = progress
        if current_repeat is not None:
            group_obj["currentRepeat"] = current_repeat
        if total_repeats is not None:
            group_obj["totalRepeats"] = total_repeats
        if autonomous_agents is not None:
            group_obj["autonomousAgents"] = autonomous_agents

        # timestamp updates
        if status == "RUNNING" and not group_obj.get("startedAt"):
            group_obj["startedAt"] = now.isoformat()
        elif status == "COMPLETED":
            group_obj["completedAt"] = now.isoformat()
        elif status == "FAILED":
            group_obj["failedAt"] = now.isoformat()
            group_obj["error"] = error
        elif status == "STOPPED":
            group_obj["stoppedAt"] = now.isoformat()
            group_obj["error"] = error

        # Write back only the updated group into the hash
        updated_group = json.dumps(group_obj)
        bytes_written = len(updated_group.encode('utf-8'))
        await self.redis.hset(groups_hash, group_field, updated_group)

        # Update meta incrementally
        try:
            meta_raw = await self.redis.hget(groups_hash, "meta")
            if meta_raw:
                meta_text = meta_raw.decode('utf-8') if isinstance(meta_raw, bytes) else meta_raw
                try:
                    meta = json.loads(meta_text)
                except Exception:
                    meta = None
            else:
                meta = None

            if meta is None:
                try:
                    all_fields = await self.redis.hgetall(groups_hash)
                    total_groups = sum(1 for k in all_fields.keys() if k.startswith("group:"))
                    completed_groups = 0
                    running_groups = 0
                    for k, v in all_fields.items():
                        if not k.startswith("group:"):
                            continue
                        val_text = v.decode('utf-8') if isinstance(v, bytes) else v
                        try:
                            go = json.loads(val_text)
                        except Exception:
                            continue
                        if go.get("status") == "COMPLETED":
                            completed_groups += 1
                        if go.get("status") == "RUNNING":
                            running_groups += 1

                    meta = {"totalGroups": total_groups, "completedGroups": completed_groups, "runningGroups": running_groups}
                except AttributeError:
                    meta = {"totalGroups": 0, "completedGroups": 0, "runningGroups": 0}

            # Adjust meta based on this update
            if status == "COMPLETED":
                meta["completedGroups"] = meta.get("completedGroups", 0) + 1
            if status == "RUNNING":
                meta["runningGroups"] = meta.get("runningGroups", 0) + 1

            await self.redis.hset(groups_hash, "meta", json.dumps(meta))
        except AttributeError:
            pass

        return {"_bytes_read": bytes_read, "_bytes_written": bytes_written}

    @measure_redis_update("parallel_simulation_update")
    async def update_parallel_simulation_status(
        self,
        simulation_id: int,
        execution_id: int,
        status: str,
        reason: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ë³‘ë ¬ íŒ¨í„´: ì „ì²´ ì‹œë®¬ë ˆì´ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        """
        execution_key = f"simulation:{simulation_id}:execution:{execution_id}"
        now = datetime.now(timezone.utc)

        # í˜„ì¬ ìƒíƒœ ì¡°íšŒ
        raw_data = await self.redis.get(execution_key)
        if not raw_data:
            return {"_bytes_read": 0, "_bytes_written": 0}

        bytes_read = len(raw_data) if isinstance(raw_data, bytes) else len(raw_data.encode('utf-8'))

        if isinstance(raw_data, bytes):
            raw_data = raw_data.decode("utf-8")

        try:
            current_status = json.loads(raw_data)
        except json.JSONDecodeError:
            return {"_bytes_read": bytes_read, "_bytes_written": 0}

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        current_status["status"] = status

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸
        current_status["timestamps"]["lastUpdated"] = now.isoformat()
        if status == "COMPLETED":
            current_status["timestamps"]["completedAt"] = now.isoformat()
        elif status == "FAILED":
            current_status["timestamps"]["failedAt"] = now.isoformat()
        elif status == "STOPPED":
            current_status["timestamps"]["stoppedAt"] = now.isoformat()

        # ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
        if reason:
            current_status["message"] = reason

        # Redisì— ì €ì¥
        updated_data = json.dumps(current_status)
        bytes_written = len(updated_data.encode('utf-8'))

        await self.redis.set(execution_key, updated_data)

        # Primary keyë„ ë™ì¼í•˜ê²Œ ì—…ë°ì´íŠ¸
        primary_key = f"simulation:{simulation_id}"
        await self.redis.set(primary_key, updated_data)

        return {
            "_bytes_read": bytes_read,
            "_bytes_written": bytes_written * 2
        }


class PerformanceReporter:
    """ì„±ëŠ¥ ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±"""

    @staticmethod
    def generate_report() -> str:
        """
        ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±

        Returns:
            ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸
        """
        summary = get_metrics_summary()

        if not summary:
            return "# âš ï¸ ì¸¡ì •ëœ ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤"

        report_lines = [
            "# ğŸš€ Redis ì—…ë°ì´íŠ¸ ì„±ëŠ¥ ê°œì„  ë¦¬í¬íŠ¸\n",
            f"**ì¸¡ì • ì‹œê°**: {datetime.now(timezone.utc).isoformat()}\n",
            "---\n"
        ]

        # ì‘ì—…ë³„ ì„±ëŠ¥ ìš”ì•½
        report_lines.append("## ğŸ“Š ì‘ì—…ë³„ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼\n")

        for operation, metrics in summary.items():
            report_lines.append(f"### {operation}\n")
            report_lines.append(f"- **í˜¸ì¶œ íšŸìˆ˜**: {metrics['í˜¸ì¶œ_íšŸìˆ˜']:,}íšŒ")
            report_lines.append(f"- **ì´ ì†Œìš”ì‹œê°„**: {metrics['ì´_ì†Œìš”ì‹œê°„_ms']:,.2f} ms")
            report_lines.append(f"- **í‰ê·  ì†Œìš”ì‹œê°„**: {metrics['í‰ê· _ì†Œìš”ì‹œê°„_ms']:.2f} ms")
            report_lines.append(f"- **ìµœì†Œ ì†Œìš”ì‹œê°„**: {metrics['ìµœì†Œ_ì†Œìš”ì‹œê°„_ms']:.2f} ms")
            report_lines.append(f"- **ìµœëŒ€ ì†Œìš”ì‹œê°„**: {metrics['ìµœëŒ€_ì†Œìš”ì‹œê°„_ms']:.2f} ms")
            report_lines.append(f"- **ì´ ì½ê¸° ë°”ì´íŠ¸**: {metrics['ì´_ì½ê¸°_ë°”ì´íŠ¸']:,} bytes ({metrics['ì´_ì½ê¸°_ë°”ì´íŠ¸'] / 1024:.2f} KB)")
            report_lines.append(f"- **í‰ê·  ì½ê¸° ë°”ì´íŠ¸**: {metrics['í‰ê· _ì½ê¸°_ë°”ì´íŠ¸']:.2f} bytes")
            report_lines.append(f"- **ì´ ì“°ê¸° ë°”ì´íŠ¸**: {metrics['ì´_ì“°ê¸°_ë°”ì´íŠ¸']:,} bytes ({metrics['ì´_ì“°ê¸°_ë°”ì´íŠ¸'] / 1024:.2f} KB)")
            report_lines.append(f"- **í‰ê·  ì“°ê¸° ë°”ì´íŠ¸**: {metrics['í‰ê· _ì“°ê¸°_ë°”ì´íŠ¸']:.2f} bytes\n")

        # ê°œì„  íš¨ê³¼ ê³„ì‚°
        report_lines.append("## ğŸ’¡ ê°œì„  íš¨ê³¼ ë¶„ì„\n")

        # Sequential vs Parallel ë¹„êµ
        seq_update = summary.get("sequential_step_update", {})
        par_update = summary.get("parallel_group_update", {})

        if seq_update and par_update:
            report_lines.append("### Sequential vs Parallel ì—…ë°ì´íŠ¸ ë¹„êµ\n")
            report_lines.append(f"- Sequential í‰ê· : {seq_update.get('í‰ê· _ì†Œìš”ì‹œê°„_ms', 0):.2f} ms")
            report_lines.append(f"- Parallel í‰ê· : {par_update.get('í‰ê· _ì†Œìš”ì‹œê°„_ms', 0):.2f} ms")

            if seq_update.get('í‰ê· _ì†Œìš”ì‹œê°„_ms', 0) > par_update.get('í‰ê· _ì†Œìš”ì‹œê°„_ms', 0):
                improvement = ((seq_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'] - par_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'])
                               / seq_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'] * 100)
                report_lines.append(f"- **Parallelì´ {improvement:.1f}% ë” ë¹ ë¦„**\n")
            else:
                improvement = ((par_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'] - seq_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'])
                               / par_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'] * 100)
                report_lines.append(f"- **Sequentialì´ {improvement:.1f}% ë” ë¹ ë¦„**\n")

        # ì „ì²´ í†µê³„
        total_operations = sum(m["í˜¸ì¶œ_íšŸìˆ˜"] for m in summary.values())
        total_time = sum(m["ì´_ì†Œìš”ì‹œê°„_ms"] for m in summary.values())
        total_read_kb = sum(m["ì´_ì½ê¸°_ë°”ì´íŠ¸"] for m in summary.values()) / 1024
        total_write_kb = sum(m["ì´_ì“°ê¸°_ë°”ì´íŠ¸"] for m in summary.values()) / 1024

        report_lines.append("### ì „ì²´ í†µê³„\n")
        report_lines.append(f"- **ì´ Redis ì‘ì—…**: {total_operations:,}íšŒ")
        report_lines.append(f"- **ì´ ì†Œìš”ì‹œê°„**: {total_time:,.2f} ms ({total_time / 1000:.2f}ì´ˆ)")
        report_lines.append(f"- **ì´ ë°ì´í„° ì½ê¸°**: {total_read_kb:,.2f} KB")
        report_lines.append(f"- **ì´ ë°ì´í„° ì“°ê¸°**: {total_write_kb:,.2f} KB")
        report_lines.append(f"- **í‰ê·  ì‘ì—… ì†ë„**: {total_time / max(total_operations, 1):.2f} ms/ì‘ì—…\n")

        # ê°œì„  ê¶Œì¥ì‚¬í•­
        report_lines.append("## ğŸ¯ ê°œì„  ê¶Œì¥ì‚¬í•­\n")

        # ëŠë¦° ì‘ì—… ì‹ë³„
        slow_operations = [
            (op, m) for op, m in summary.items()
            if m.get("í‰ê· _ì†Œìš”ì‹œê°„_ms", 0) > 10  # 10ms ì´ìƒ
        ]

        if slow_operations:
            report_lines.append("### âš ï¸ ê°œì„ ì´ í•„ìš”í•œ ì‘ì—… (í‰ê·  10ms ì´ìƒ)\n")
            for op, metrics in slow_operations:
                report_lines.append(
                    f"- **{op}**: {metrics['í‰ê· _ì†Œìš”ì‹œê°„_ms']:.2f} ms "
                    f"(í˜¸ì¶œ {metrics['í˜¸ì¶œ_íšŸìˆ˜']}íšŒ)"
                )
            report_lines.append("")

        # ë°ì´í„° ì „ì†¡ëŸ‰ì´ ë§ì€ ì‘ì—…
        heavy_operations = [
            (op, m) for op, m in summary.items()
            if m.get("í‰ê· _ì“°ê¸°_ë°”ì´íŠ¸", 0) > 5000  # 5KB ì´ìƒ
        ]

        if heavy_operations:
            report_lines.append("### ğŸ“¦ ë°ì´í„° ì „ì†¡ëŸ‰ì´ ë§ì€ ì‘ì—… (í‰ê·  5KB ì´ìƒ)\n")
            for op, metrics in heavy_operations:
                report_lines.append(
                    f"- **{op}**: {metrics['í‰ê· _ì“°ê¸°_ë°”ì´íŠ¸'] / 1024:.2f} KB "
                    f"(í˜¸ì¶œ {metrics['í˜¸ì¶œ_íšŸìˆ˜']}íšŒ)"
                )
            report_lines.append("")

        report_lines.append("---\n")
        report_lines.append("*ì´ ë¦¬í¬íŠ¸ëŠ” ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤*")

        return "\n".join(report_lines)

    @measure_redis_update("sequential_simulation_update")
    async def update_sequential_simulation_status(
        self,
        simulation_id: int,
        execution_id: int,
        status: str,
        reason: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ìˆœì°¨ íŒ¨í„´: ì „ì²´ ì‹œë®¬ë ˆì´ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        """
        execution_key = f"simulation:{simulation_id}:execution:{execution_id}"
        now = datetime.now(timezone.utc)

        # í˜„ì¬ ìƒíƒœ ì¡°íšŒ
        raw_data = await self.redis.get(execution_key)
        if not raw_data:
            return {"_bytes_read": 0, "_bytes_written": 0}

        bytes_read = len(raw_data) if isinstance(raw_data, bytes) else len(raw_data.encode('utf-8'))

        if isinstance(raw_data, bytes):
            raw_data = raw_data.decode("utf-8")

        try:
            current_status = json.loads(raw_data)
        except json.JSONDecodeError:
            return {"_bytes_read": bytes_read, "_bytes_written": 0}

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        current_status["status"] = status

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸
        current_status["timestamps"]["lastUpdated"] = now.isoformat()
        if status == "COMPLETED":
            current_status["timestamps"]["completedAt"] = now.isoformat()
        elif status == "FAILED":
            current_status["timestamps"]["failedAt"] = now.isoformat()
        elif status == "STOPPED":
            current_status["timestamps"]["stoppedAt"] = now.isoformat()

        # ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
        if reason:
            current_status["message"] = reason

        # Redisì— ì €ì¥
        updated_data = json.dumps(current_status)
        bytes_written = len(updated_data.encode('utf-8'))

        await self.redis.set(execution_key, updated_data)

        # Primary keyë„ ë™ì¼í•˜ê²Œ ì—…ë°ì´íŠ¸
        primary_key = f"simulation:{simulation_id}"
        await self.redis.set(primary_key, updated_data)

        return {
            "_bytes_read": bytes_read,
            "_bytes_written": bytes_written * 2
        }

    # ========================================
    # Parallel íŒ¨í„´ìš© ìµœì í™”
    # ========================================

    @measure_redis_update("parallel_group_update")
    async def update_parallel_group_status(
        self,
        simulation_id: int,
        execution_id: int,
        group_id: int,
        status: str,
        progress: Optional[float] = None,
        current_repeat: Optional[int] = None,
        total_repeats: Optional[int] = None,
        autonomous_agents: Optional[int] = None,
        error: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ë³‘ë ¬ íŒ¨í„´: Group ìƒíƒœë§Œ ì—…ë°ì´íŠ¸ (ë¶€ë¶„ ì—…ë°ì´íŠ¸)

        ê°œì„  ë°©ì‹:
        - ì „ì²´ ë°ì´í„° ëŒ€ì‹  groupDetails ë°°ì—´ë§Œ ì¡°íšŒ
        - í•´ë‹¹ groupë§Œ ìˆ˜ì • í›„ ë‹¤ì‹œ ì €ì¥
        """
        execution_key = f"simulation:{simulation_id}:execution:{execution_id}"
        groups_hash = f"{execution_key}:groups"
        now = datetime.now(timezone.utc)

        group_field = f"group:{group_id}"

        # Try to get the individual group from a hash (partial-update storage)
        try:
            group_raw = await self.redis.hget(groups_hash, group_field)
        except AttributeError:
            group_raw = None

        # If group not present in hash, try to migrate from full execution JSON
        if not group_raw:
            full_raw = await self.redis.get(execution_key)
            if not full_raw:
                return {"_bytes_read": 0, "_bytes_written": 0}

            full_raw_text = full_raw.decode('utf-8') if isinstance(full_raw, bytes) else full_raw
            try:
                full_obj = json.loads(full_raw_text)
            except json.JSONDecodeError:
                return {"_bytes_read": len(full_raw_text.encode('utf-8')), "_bytes_written": 0}

            # Migrate each group into hash fields
            group_details = full_obj.get("groupDetails", [])
            for g in group_details:
                field = f"group:{g.get('groupId')}"
                await self.redis.hset(groups_hash, field, json.dumps(g))

            # after migration, get the specific group
            group_raw = await self.redis.hget(groups_hash, group_field)

        if not group_raw:
            debug_print(f"âš ï¸ Group {group_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return {"_bytes_read": 0, "_bytes_written": 0}

        # Normalize raw bytes/str
        group_text = group_raw.decode('utf-8') if isinstance(group_raw, bytes) else group_raw
        bytes_read = len(group_text.encode('utf-8'))

        try:
            group_obj = json.loads(group_text)
        except json.JSONDecodeError:
            return {"_bytes_read": bytes_read, "_bytes_written": 0}

        # Update group object in-place
        group_obj["status"] = status
        if progress is not None:
            group_obj["progress"] = progress
        if current_repeat is not None:
            group_obj["currentRepeat"] = current_repeat
        if total_repeats is not None:
            group_obj["totalRepeats"] = total_repeats
        if autonomous_agents is not None:
            group_obj["autonomousAgents"] = autonomous_agents

        # timestamp updates
        if status == "RUNNING" and not group_obj.get("startedAt"):
            group_obj["startedAt"] = now.isoformat()
        elif status == "COMPLETED":
            group_obj["completedAt"] = now.isoformat()
        elif status == "FAILED":
            group_obj["failedAt"] = now.isoformat()
            group_obj["error"] = error
        elif status == "STOPPED":
            group_obj["stoppedAt"] = now.isoformat()
            group_obj["error"] = error

        # Write back only the updated group into the hash
        updated_group = json.dumps(group_obj)
        bytes_written = len(updated_group.encode('utf-8'))
        await self.redis.hset(groups_hash, group_field, updated_group)

        # Update meta incrementally
        try:
            meta_raw = await self.redis.hget(groups_hash, "meta")
            if meta_raw:
                meta_text = meta_raw.decode('utf-8') if isinstance(meta_raw, bytes) else meta_raw
                try:
                    meta = json.loads(meta_text)
                except Exception:
                    meta = None
            else:
                meta = None

            if meta is None:
                try:
                    all_fields = await self.redis.hgetall(groups_hash)
                    total_groups = sum(1 for k in all_fields.keys() if k.startswith("group:"))
                    completed_groups = 0
                    running_groups = 0
                    for k, v in all_fields.items():
                        if not k.startswith("group:"):
                            continue
                        val_text = v.decode('utf-8') if isinstance(v, bytes) else v
                        try:
                            go = json.loads(val_text)
                        except Exception:
                            continue
                        if go.get("status") == "COMPLETED":
                            completed_groups += 1
                        if go.get("status") == "RUNNING":
                            running_groups += 1

                    meta = {"totalGroups": total_groups, "completedGroups": completed_groups, "runningGroups": running_groups}
                except AttributeError:
                    meta = {"totalGroups": 0, "completedGroups": 0, "runningGroups": 0}

            # Adjust meta based on this update
            if status == "COMPLETED":
                meta["completedGroups"] = meta.get("completedGroups", 0) + 1
            if status == "RUNNING":
                meta["runningGroups"] = meta.get("runningGroups", 0) + 1

            await self.redis.hset(groups_hash, "meta", json.dumps(meta))
        except AttributeError:
            pass

        return {"_bytes_read": bytes_read, "_bytes_written": bytes_written}

    @measure_redis_update("parallel_simulation_update")
    async def update_parallel_simulation_status(
        self,
        simulation_id: int,
        execution_id: int,
        status: str,
        reason: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ë³‘ë ¬ íŒ¨í„´: ì „ì²´ ì‹œë®¬ë ˆì´ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        """
        execution_key = f"simulation:{simulation_id}:execution:{execution_id}"
        now = datetime.now(timezone.utc)

        # í˜„ì¬ ìƒíƒœ ì¡°íšŒ
        raw_data = await self.redis.get(execution_key)
        if not raw_data:
            return {"_bytes_read": 0, "_bytes_written": 0}

        bytes_read = len(raw_data) if isinstance(raw_data, bytes) else len(raw_data.encode('utf-8'))

        if isinstance(raw_data, bytes):
            raw_data = raw_data.decode("utf-8")

        try:
            current_status = json.loads(raw_data)
        except json.JSONDecodeError:
            return {"_bytes_read": bytes_read, "_bytes_written": 0}

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        current_status["status"] = status

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸
        current_status["timestamps"]["lastUpdated"] = now.isoformat()
        if status == "COMPLETED":
            current_status["timestamps"]["completedAt"] = now.isoformat()
        elif status == "FAILED":
            current_status["timestamps"]["failedAt"] = now.isoformat()
        elif status == "STOPPED":
            current_status["timestamps"]["stoppedAt"] = now.isoformat()

        # ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
        if reason:
            current_status["message"] = reason

        # Redisì— ì €ì¥
        updated_data = json.dumps(current_status)
        bytes_written = len(updated_data.encode('utf-8'))

        await self.redis.set(execution_key, updated_data)

        # Primary keyë„ ë™ì¼í•˜ê²Œ ì—…ë°ì´íŠ¸
        primary_key = f"simulation:{simulation_id}"
        await self.redis.set(primary_key, updated_data)

        return {
            "_bytes_read": bytes_read,
            "_bytes_written": bytes_written * 2
        }


class PerformanceReporter:
    """ì„±ëŠ¥ ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±"""

    @staticmethod
    def generate_report() -> str:
        """
        ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±

        Returns:
            ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸
        """
        summary = get_metrics_summary()

        if not summary:
            return "# âš ï¸ ì¸¡ì •ëœ ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤"

        report_lines = [
            "# ğŸš€ Redis ì—…ë°ì´íŠ¸ ì„±ëŠ¥ ê°œì„  ë¦¬í¬íŠ¸\n",
            f"**ì¸¡ì • ì‹œê°**: {datetime.now(timezone.utc).isoformat()}\n",
            "---\n"
        ]

        # ì‘ì—…ë³„ ì„±ëŠ¥ ìš”ì•½
        report_lines.append("## ğŸ“Š ì‘ì—…ë³„ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼\n")

        for operation, metrics in summary.items():
            report_lines.append(f"### {operation}\n")
            report_lines.append(f"- **í˜¸ì¶œ íšŸìˆ˜**: {metrics['í˜¸ì¶œ_íšŸìˆ˜']:,}íšŒ")
            report_lines.append(f"- **ì´ ì†Œìš”ì‹œê°„**: {metrics['ì´_ì†Œìš”ì‹œê°„_ms']:,.2f} ms")
            report_lines.append(f"- **í‰ê·  ì†Œìš”ì‹œê°„**: {metrics['í‰ê· _ì†Œìš”ì‹œê°„_ms']:.2f} ms")
            report_lines.append(f"- **ìµœì†Œ ì†Œìš”ì‹œê°„**: {metrics['ìµœì†Œ_ì†Œìš”ì‹œê°„_ms']:.2f} ms")
            report_lines.append(f"- **ìµœëŒ€ ì†Œìš”ì‹œê°„**: {metrics['ìµœëŒ€_ì†Œìš”ì‹œê°„_ms']:.2f} ms")
            report_lines.append(f"- **ì´ ì½ê¸° ë°”ì´íŠ¸**: {metrics['ì´_ì½ê¸°_ë°”ì´íŠ¸']:,} bytes ({metrics['ì´_ì½ê¸°_ë°”ì´íŠ¸'] / 1024:.2f} KB)")
            report_lines.append(f"- **í‰ê·  ì½ê¸° ë°”ì´íŠ¸**: {metrics['í‰ê· _ì½ê¸°_ë°”ì´íŠ¸']:.2f} bytes")
            report_lines.append(f"- **ì´ ì“°ê¸° ë°”ì´íŠ¸**: {metrics['ì´_ì“°ê¸°_ë°”ì´íŠ¸']:,} bytes ({metrics['ì´_ì“°ê¸°_ë°”ì´íŠ¸'] / 1024:.2f} KB)")
            report_lines.append(f"- **í‰ê·  ì“°ê¸° ë°”ì´íŠ¸**: {metrics['í‰ê· _ì“°ê¸°_ë°”ì´íŠ¸']:.2f} bytes\n")

        # ê°œì„  íš¨ê³¼ ê³„ì‚°
        report_lines.append("## ğŸ’¡ ê°œì„  íš¨ê³¼ ë¶„ì„\n")

        # Sequential vs Parallel ë¹„êµ
        seq_update = summary.get("sequential_step_update", {})
        par_update = summary.get("parallel_group_update", {})

        if seq_update and par_update:
            report_lines.append("### Sequential vs Parallel ì—…ë°ì´íŠ¸ ë¹„êµ\n")
            report_lines.append(f"- Sequential í‰ê· : {seq_update.get('í‰ê· _ì†Œìš”ì‹œê°„_ms', 0):.2f} ms")
            report_lines.append(f"- Parallel í‰ê· : {par_update.get('í‰ê· _ì†Œìš”ì‹œê°„_ms', 0):.2f} ms")

            if seq_update.get('í‰ê· _ì†Œìš”ì‹œê°„_ms', 0) > par_update.get('í‰ê· _ì†Œìš”ì‹œê°„_ms', 0):
                improvement = ((seq_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'] - par_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'])
                               / seq_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'] * 100)
                report_lines.append(f"- **Parallelì´ {improvement:.1f}% ë” ë¹ ë¦„**\n")
            else:
                improvement = ((par_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'] - seq_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'])
                               / par_update['í‰ê· _ì†Œìš”ì‹œê°„_ms'] * 100)
                report_lines.append(f"- **Sequentialì´ {improvement:.1f}% ë” ë¹ ë¦„**\n")

        # ì „ì²´ í†µê³„
        total_operations = sum(m["í˜¸ì¶œ_íšŸìˆ˜"] for m in summary.values())
        total_time = sum(m["ì´_ì†Œìš”ì‹œê°„_ms"] for m in summary.values())
        total_read_kb = sum(m["ì´_ì½ê¸°_ë°”ì´íŠ¸"] for m in summary.values()) / 1024
        total_write_kb = sum(m["ì´_ì“°ê¸°_ë°”ì´íŠ¸"] for m in summary.values()) / 1024

        report_lines.append("### ì „ì²´ í†µê³„\n")
        report_lines.append(f"- **ì´ Redis ì‘ì—…**: {total_operations:,}íšŒ")
        report_lines.append(f"- **ì´ ì†Œìš”ì‹œê°„**: {total_time:,.2f} ms ({total_time / 1000:.2f}ì´ˆ)")
        report_lines.append(f"- **ì´ ë°ì´í„° ì½ê¸°**: {total_read_kb:,.2f} KB")
        report_lines.append(f"- **ì´ ë°ì´í„° ì“°ê¸°**: {total_write_kb:,.2f} KB")
        report_lines.append(f"- **í‰ê·  ì‘ì—… ì†ë„**: {total_time / max(total_operations, 1):.2f} ms/ì‘ì—…\n")

        # ê°œì„  ê¶Œì¥ì‚¬í•­
        report_lines.append("## ğŸ¯ ê°œì„  ê¶Œì¥ì‚¬í•­\n")

        # ëŠë¦° ì‘ì—… ì‹ë³„
        slow_operations = [
            (op, m) for op, m in summary.items()
            if m.get("í‰ê· _ì†Œìš”ì‹œê°„_ms", 0) > 10  # 10ms ì´ìƒ
        ]

        if slow_operations:
            report_lines.append("### âš ï¸ ê°œì„ ì´ í•„ìš”í•œ ì‘ì—… (í‰ê·  10ms ì´ìƒ)\n")
            for op, metrics in slow_operations:
                report_lines.append(
                    f"- **{op}**: {metrics['í‰ê· _ì†Œìš”ì‹œê°„_ms']:.2f} ms "
                    f"(í˜¸ì¶œ {metrics['í˜¸ì¶œ_íšŸìˆ˜']}íšŒ)"
                )
            report_lines.append("")

        # ë°ì´í„° ì „ì†¡ëŸ‰ì´ ë§ì€ ì‘ì—…
        heavy_operations = [
            (op, m) for op, m in summary.items()
            if m.get("í‰ê· _ì“°ê¸°_ë°”ì´íŠ¸", 0) > 5000  # 5KB ì´ìƒ
        ]

        if heavy_operations:
            report_lines.append("### ğŸ“¦ ë°ì´í„° ì „ì†¡ëŸ‰ì´ ë§ì€ ì‘ì—… (í‰ê·  5KB ì´ìƒ)\n")
            for op, metrics in heavy_operations:
                report_lines.append(
                    f"- **{op}**: {metrics['í‰ê· _ì“°ê¸°_ë°”ì´íŠ¸'] / 1024:.2f} KB "
                    f"(í˜¸ì¶œ {metrics['í˜¸ì¶œ_íšŸìˆ˜']}íšŒ)"
                )
            report_lines.append("")

        report_lines.append("---\n")
        report_lines.append("*ì´ ë¦¬í¬íŠ¸ëŠ” ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤*")

        return "\n".join(report_lines)
